# Neural Network from Scratch

This notebook demonstrates how to build, train, and evaluate a neural network from scratch using only Python and NumPy.  
It is designed for educational purposes to understand how neural networks work internally without using frameworks like TensorFlow or PyTorch.

## Repository Structure

- neural_network_from_scratch.ipynb (this notebook)

## Overview

The notebook covers:
- Initializing network parameters
- Forward and backward propagation
- Activation functions (sigmoid, softmax) and cross-entropy loss
- Training with gradient descent
- Visualizing training loss
- Evaluating accuracy on the Iris dataset

## Learning Objectives

- Understand the structure of a simple neural network
- Implement forward and backward propagation
- Compute gradients and update weights manually
- Apply activation and loss functions
- Train and test a simple neural network

## How to Use

1. Open in Google Colab (recommended)
2. Run all cells sequentially
3. Experiment with learning rate, layers, or activation functions

## Requirements

- Python 3.8+
- NumPy
- Matplotlib

## Next Steps

- Experiment with ReLU or tanh activation
- Add hidden layers
- Implement Adam or momentum optimizer
- Apply to other datasets like MNIST

## License

Open-source and free for educational use.
